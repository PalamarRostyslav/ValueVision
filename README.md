# ValueVision ğŸ¯

**AI-Powered Price Prediction System for Amazon Products**

A comprehensive end-to-end machine learning platform that processes Amazon product data, trains AI models, and provides intelligent price predictions. 
ValueVision combines advanced data processing, multiple AI fine-tuning strategies, and intelligent model management.

## ğŸŒŸ Overview

ValueVision is a complete ML/AI platform that transforms raw Amazon product data into intelligent price prediction models. It features:

- **ğŸ”„ Intelligent Data Pipeline**: Processes millions of Amazon products with smart sampling and balancing
- **ğŸ¤– Multiple AI Strategies**: OpenAI GPT fine-tuning, Random Forest, feature-based models, and more
- **ğŸ’¾ Smart Model Management**: Automatic model persistence, reuse, and cost optimization
- **ğŸ“Š Advanced Analytics**: Comprehensive visualizations and performance metrics
- **âš¡ Production Ready**: Scalable architecture with multi-processing and optimization features

  ## ğŸ—ï¸ Architecture & Project Structure

```
ValueVision/
â”œâ”€â”€ ğŸ¯ CORE PIPELINE
â”‚   â”œâ”€â”€ main.py                   # Main orchestrator with 15+ command options
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py           # Centralized configuration with .env support
â”‚   â””â”€â”€ .env.example             # Environment variables template
â”‚
â”œâ”€â”€ ğŸ“Š DATA PROCESSING ENGINE
â”‚   â”œâ”€â”€ src/data/
â”‚   â”‚   â”œâ”€â”€ pipeline.py           # Smart data processing pipeline
â”‚   â”‚   â”œâ”€â”€ loaders.py            # Multi-format dataset loaders
â”‚   â”‚   â”œâ”€â”€ sampling.py           # Intelligent sampling algorithms
â”‚   â”‚   â”œâ”€â”€ dataset_creator.py    # Multi-format export (Pickle, HuggingFace)
â”‚   â”‚   â””â”€â”€ models.py             # Data models and schemas
â”‚   â”‚
â”‚   â”œâ”€â”€ src/visualization/
â”‚   â”‚   â””â”€â”€ visualizer.py         # Advanced analytics and plotting
â”‚   â”‚
â”‚   â””â”€â”€ src/utils/
â”‚       â”œâ”€â”€ environment.py        # Environment setup and authentication
â”‚       â””â”€â”€ testing.py            # Testing utilities
â”‚
â”œâ”€â”€ ğŸ¤– AI & MACHINE LEARNING
â”‚   â”œâ”€â”€ src/ai_models/
â”‚   â”‚   â”œâ”€â”€ base.py               # Abstract base for AI models
â”‚   â”‚   â”œâ”€â”€ openai_model.py       # OpenAI GPT fine-tuning implementation
â”‚   â”‚   â””â”€â”€ model_manager.py      # Smart model persistence & reuse system
â”‚   â”‚
â”‚   â”œâ”€â”€ src/testing/
â”‚   â”‚   â”œâ”€â”€ tester.py             # Model evaluation framework
â”‚   â”‚   â”œâ”€â”€ fine_tuning.py        # Fine-tuning base classes
â”‚   â”‚   â””â”€â”€ strategies/           # Multiple AI training strategies
â”‚   â”‚       â”œâ”€â”€ openai_strategy.py       # OpenAI GPT fine-tuning
â”‚   â”‚       â”œâ”€â”€ random_forest.py         # Random Forest + Word2Vec
â”‚   â”‚       â”œâ”€â”€ feature_based.py         # Feature engineering approach
â”‚   â”‚       â””â”€â”€ random_seed.py           # Seed optimization
â”‚   â”‚
â”œâ”€â”€ ğŸ“ OUTPUT & DATA
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ models/              # AI model registry and metadata
â”‚   â”‚   â”œâ”€â”€ fine_tuning/         # Training files and logs
â”‚   â”‚   â”œâ”€â”€ train.pkl            # Processed training dataset
â”‚   â”‚   â”œâ”€â”€ test.pkl             # Processed test dataset
â”‚   â”‚   â””â”€â”€ dataset/             # HuggingFace format exports
â”‚   â”‚
â””â”€â”€ ğŸ”§ UTILITIES & COMPATIBILITY
    â”œâ”€â”€ items.py                 # Backward compatibility for pickle loading
    â”œâ”€â”€ data_viewer.py          # Legacy data viewer (maintained)
    â”œâ”€â”€ test_*.py               # Comprehensive test suites
    â””â”€â”€ requirements.txt         # Python dependencies
```


## ğŸš€ Core Features

### ğŸ“Š **Data Processing Engine**

- **Multi-Source Loading**: Process 5+ Amazon product categories simultaneously
- **Intelligent Sampling**: Price-based and category-balanced sampling algorithms
- **Smart Data Balancing**: Ensures representative datasets across price ranges
- **Multi-Format Export**: Pickle, HuggingFace datasets, JSON formats
- **Memory Optimization**: Efficient processing of millions of products
- **Parallel Processing**: Multi-core optimization for large datasets

### ğŸ¤– **AI & Machine Learning**

- **OpenAI GPT Fine-Tuning**: State-of-the-art language model fine-tuning for price prediction
- **Multiple AI Strategies**: Random Forest, Feature Engineering, Seed Optimization
- **Smart Model Management**: Automatic model persistence, reuse, and cost optimization
- **Performance Analytics**: Comprehensive model evaluation and metrics
- **Hyperparameter Optimization**: Automated parameter tuning
- **Cross-Validation**: Robust model validation techniques

### ğŸ’¾ **Model Management System**

- **ğŸ”„ Automatic Model Reuse**: Prevents costly retraining of identical models
- **ğŸ’° Cost Optimization**: Save money by reusing existing fine-tuned models
- **ğŸ“‹ Model Registry**: Track all models with metadata, metrics, and versioning
- **âš¡ Instant Deployment**: Use specific models immediately with model ID specification
- **ğŸ“ˆ Performance Tracking**: Monitor model usage and effectiveness over time

### ğŸ“Š **Advanced Analytics**

- **Rich Visualizations**: Price distributions, category analysis, correlation matrices
- **Performance Metrics**: RMSE, MAE, accuracy scores, error analysis
- **Data Quality Insights**: Missing data analysis, outlier detection
- **Training Progress**: Real-time monitoring of model training
- **Comparative Analysis**: Side-by-side model performance comparison

## ğŸ› ï¸ Installation & Setup

### Prerequisites

- Python 3.8+
- OpenAI API access (for GPT fine-tuning)
- HuggingFace account (for dataset access)
- 8GB+ RAM recommended for large datasets

### Quick Installation

1. **Clone the repository:**

```bash
git clone https://github.com/PalamarRostyslav/ValueVision.git
cd ValueVision
```

2. **Install dependencies:**

```bash
pip install -r requirements.txt
```

3. **Set up environment variables:**

```bash
# Copy the example environment file
cp .env.example .env

# Edit .env with your API keys
# Required for AI fine-tuning:
OPENAI_API_KEY=your_openai_api_key
HF_TOKEN=your_huggingface_token
```

4. **Verify installation:**

```bash
python main.py --help
```


## ğŸ¯ Usage Guide

### ğŸš€ Quick Start Commands

```bash
# ğŸ”¥ FASTEST: Use existing processed data
python main.py --use-existing

# ğŸ“Š ANALYSIS: Quick data exploration
python main.py --analysis-only

# ğŸ”„ FULL: Complete pipeline from scratch
python main.py

# ğŸ¯ SPECIFIC: Process only certain categories
python main.py --datasets Electronics Automotive Home
```

### ğŸ¤– AI Fine-Tuning & Model Training

#### OpenAI GPT Fine-Tuning

```bash
# ğŸ†• Train new OpenAI model (reuses existing if identical data)
python main.py --fine-tune openai

# ğŸ’° Force new training (ignores existing models)
python main.py --fine-tune openai --force-new-model

# âš¡ Use specific pre-trained model instantly
python main.py --fine-tune openai --model-id ft:gpt-4o-mini:your-org:model:abc123
```

#### Alternative AI Strategies

```bash
# ğŸŒ² Random Forest with Word2Vec
python main.py --fine-tune random-forest

# âš™ï¸ Feature-based machine learning
python main.py --fine-tune feature-based

# ğŸ² Random seed optimization
python main.py --fine-tune random-seed
```

### ğŸ’¾ Model Management

#### View & Manage Models

```bash
# ğŸ“‹ List all saved models with details
python main.py --list-models

# ğŸ§¹ Clean up old models (keeps latest 3 per provider)
python main.py --cleanup-models 30  # Remove models older than 30 days

# ğŸ“Š Test existing models
python main.py --test --test-size 100
```

#### Cost-Saving Model Reuse

```bash
# First run: Trains and saves model
python main.py --fine-tune openai
# Output: "âœ“ Model saved: ft:gpt-4o-mini:org:model:abc123"

# Subsequent runs: Instant reuse (no cost!)
python main.py --fine-tune openai
# Output: "âœ“ Found existing model, reusing..."

# Manual model specification
python main.py --fine-tune openai --model-id ft:gpt-4o-mini:org:model:abc123
```

### ğŸ”§ Advanced Usage

#### Custom Data Processing

```bash
# Force reload specific datasets
python main.py --datasets Electronics --force-reload

# Use existing data but force new analysis
python main.py --use-existing --analysis-only

# Custom test size for model evaluation
python main.py --test --test-size 500
```

#### Development Workflow

```bash
# 1. Initial setup (full pipeline)
python main.py

# 2. Model experimentation (fast iterations)
python main.py --use-existing --fine-tune openai

# 3. Production deployment
python main.py --fine-tune openai --model-id your_best_model_id
```

## ğŸ“Š Complete Command Reference

| Command                     | Description                     | Example                                                               |
| --------------------------- | ------------------------------- | --------------------------------------------------------------------- |
| **Data Processing**         |
| `python main.py`            | Full pipeline from scratch      | `python main.py`                                                      |
| `--use-existing`            | Use cached data if available    | `python main.py --use-existing`                                       |
| `--analysis-only`           | Quick data analysis only        | `python main.py --analysis-only`                                      |
| `--force-reload`            | Force reload even with existing | `python main.py --force-reload`                                       |
| `--datasets LIST`           | Process specific categories     | `python main.py --datasets Electronics Home`                          |
| **AI Fine-Tuning**          |
| `--fine-tune openai`        | OpenAI GPT fine-tuning          | `python main.py --fine-tune openai`                                   |
| `--fine-tune random-forest` | Random Forest training          | `python main.py --fine-tune random-forest`                            |
| `--fine-tune feature-based` | Feature engineering approach    | `python main.py --fine-tune feature-based`                            |
| `--fine-tune random-seed`   | Seed optimization               | `python main.py --fine-tune random-seed`                              |
| **Model Management**        |
| `--model-id ID`             | Use specific model              | `python main.py --fine-tune openai --model-id ft:gpt-4o-mini:org:abc` |
| `--force-new-model`         | Force new training              | `python main.py --fine-tune openai --force-new-model`                 |
| `--list-models`             | Show all saved models           | `python main.py --list-models`                                        |
| `--cleanup-models DAYS`     | Remove old models               | `python main.py --cleanup-models 30`                                  |
| **Testing & Evaluation**    |
| `--test`                    | Run model testing               | `python main.py --test`                                               |
| `--test-size N`             | Custom test size                | `python main.py --test --test-size 500`                               |

## ğŸ’° Cost Optimization Features

### Smart Model Reuse

- **Automatic Detection**: Identifies identical training configurations
- **Zero-Cost Reuse**: Skip expensive retraining for same data
- **Model Registry**: Track and version all trained models
- **Intelligent Matching**: Hash-based training data comparison

## ğŸ”§ Advanced Configuration

### Core Settings (`config/settings.py`)

```python
# Data Processing
DATASET_NAMES = ['Electronics', 'Automotive', 'Home', ...]  # Categories to process
TRAIN_SIZE = 400000                    # Training dataset size
TEST_SIZE = 2000                      # Test dataset size
MAX_ITEMS_PER_PRICE = 1200           # Price point balancing
RANDOM_SEED = 42                     # Reproducibility

# AI Model Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")      # Auto-loaded from .env
OPENAI_MODEL = "gpt-4o-mini-2024-07-18"          # Default OpenAI model
HF_TOKEN = os.getenv("HF_TOKEN")                  # HuggingFace access

# Advanced Processing
USE_MULTIPROCESSING = True           # Enable parallel processing
MEMORY_OPTIMIZATION = True           # Optimize for large datasets
VERBOSE_LOGGING = True              # Detailed operation logs
```

### Environment Variables (`.env`)

```bash
# Required for AI fine-tuning
OPENAI_API_KEY=sk-your-openai-key
HF_TOKEN=hf_your-huggingface-token

# Optional: Custom model preferences
OPENAI_MODEL=gpt-4o-mini-2024-07-18
DEFAULT_TEST_SIZE=250
```

### Programmatic Usage

```python
# Import core modules
from src.data.pipeline import DataPipeline
from src.ai_models.openai_model import OpenAIFineTuning
from src.ai_models.model_manager import ModelManager

# Data Processing
pipeline = DataPipeline(use_existing_data=True)
train_data, test_data = pipeline.load_processed_data()

# AI Model Training
openai_model = OpenAIFineTuning(model_name="gpt-4o-mini-2024-07-18")
predictor = openai_model.fine_tune_and_create_predictor(train_data, test_data)

# Model Management
manager = ModelManager()
models = manager.list_models(provider="openai")
existing_model = manager.find_existing_model("openai", "gpt-4o-mini", training_info)

# Make Predictions
price_prediction = predictor(item)
```

## ğŸ”„ Data Processing Pipeline

### Pipeline Stages

1. **ğŸ”§ Environment Setup**

   - Load environment variables and API keys
   - Authenticate with HuggingFace and OpenAI
   - Initialize logging and configuration

2. **ğŸ“¥ Multi-Source Data Loading**

   - Download Amazon product datasets (10+ categories)
   - Handle various data formats and schemas
   - Memory-efficient streaming for large datasets

3. **ğŸ” Data Analysis & Validation**

   - Generate overview statistics and visualizations
   - Identify data quality issues and outliers
   - Create price distribution and category analysis

4. **âš–ï¸ Intelligent Sampling**

   - Price-based balancing to prevent bias
   - Category-weighted sampling for representation
   - Outlier handling and data cleaning

5. **âœ‚ï¸ Train/Test Splitting**

   - Stratified splitting to maintain distributions
   - Configurable split ratios and sizes
   - Reproducible splits with seed control

6. **ğŸ’¾ Multi-Format Export**

   - Pickle files for Python compatibility
   - HuggingFace datasets for ML frameworks
   - JSON exports for external tools

7. **ğŸ“Š Final Analytics**
   - Generate comprehensive data reports
   - Performance metrics and statistics
   - Visual summaries and insights

### Data Flow Architecture

```
Raw Amazon Data â†’ Loading â†’ Validation â†’ Sampling â†’ Splitting â†’ Export
                     â†“         â†“          â†“         â†“         â†“
                 Analytics  Quality   Balancing  Testing   Multi-Format
                           Checks               Validation    Output
```

## ğŸ¤– AI Fine-Tuning Strategies

### 1. ğŸ¯ **OpenAI GPT Fine-Tuning** (Recommended)

- **Technology**: GPT-4o-mini fine-tuning via OpenAI API
- **Advantages**: State-of-the-art accuracy, handles complex product descriptions
- **Use Case**: Production-grade price prediction with natural language understanding
- **Cost**: ~$1-5 per training job (with smart reuse system)
- 
<img width="1197" height="861" alt="Pasted image 20250820203345" src="https://github.com/user-attachments/assets/47934cd4-4c9d-4eb2-b9d6-6b9fb0abf1c9" />

### 2. ğŸŒ² **Random Forest + Word2Vec**

- **Technology**: Ensemble learning with semantic embeddings
- **Advantages**: Fast training, interpretable features, good baseline
- **Use Case**: Quick prototyping and feature importance analysis
- **Cost**: Free, uses local computation

<img width="1197" height="859" alt="Pasted image 20250820190405" src="https://github.com/user-attachments/assets/a69c7832-0dc9-4606-a155-aa639ba4dbc8" />


### 3. âš™ï¸ **Feature-Based ML**

- **Technology**: Traditional ML with engineered features
- **Advantages**: Highly interpretable, fast inference, low resource usage
- **Use Case**: Scenarios requiring model explainability
- **Cost**: Free, minimal computational requirements
- 
<img width="1199" height="860" alt="Pasted image 20250820183635" src="https://github.com/user-attachments/assets/defffa4a-39d4-464c-b05d-f63cfc97f888" />

### 4. ğŸ² **Random Seed Optimization**

- **Technology**: Hyperparameter optimization through seed tuning
- **Advantages**: Improves any model's performance, no additional complexity
- **Use Case**: Maximizing performance of existing models
- **Cost**: Free, automated optimization
- 
<img width="1199" height="864" alt="Pasted image 20250820183656" src="https://github.com/user-attachments/assets/8812ab10-ef56-481c-b950-820462f9aa27" />

### Performance Comparison

| Strategy          | Accuracy     | Speed      | Cost     | Interpretability | Best For   |
| ----------------- | -----------  | ---------- | -------  | ---------------- | ---------- |
| OpenAI GPT        | ğŸŸ¢ Highest  | ğŸŸ¡ Medium  | ğŸŸ¡ Paid | ğŸ”´ Low           | Production |
| Random Forest     | ğŸŸ¡ Good     | ğŸŸ¢ Fast    | ğŸŸ¢ Free | ğŸŸ¢ High          | Baseline   |
| Feature-Based     | ğŸ”´ Low      | ğŸŸ¢ Fastest | ğŸŸ¢ Free | ğŸŸ¢ Highest       | Analysis   |
| Seed Optimization | ğŸ”´ Low      | ğŸŸ¢ Fast    | ğŸŸ¢ Free | ğŸŸ¢ High          | Tuning     |

### Performance Tips

- **ğŸ’¾ Use `--use-existing`** for faster iterations during development
- **ğŸ¯ Start with smaller datasets** to validate workflows
- **âš¡ Use specific model IDs** for production to avoid retraining



